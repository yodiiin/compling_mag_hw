{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1\n",
    "Попробуем себя в решении задачи определения темы текста. Будем считать, что два текста похожи по теме, если у них больше общих слов (только не предлогов с союзами), чем у других текстов. У нашей программы для определения темы будет несколько готовых текстов (достаточно больших!) с уже известной темой в базе: выберите тексты (и темы) самостоятельно, 5-6 будет достаточно.\n",
    "\n",
    "Что должна делать программа? При запуске вы ей сообщаете название нового файла с текстом, который нужно классифицировать, она его открывает, обрабатывает и сравнивает с текстами в своей базе. С которым из текстов оказалось больше всего общих слов, того и тема! Очевидно, вам понадобится какие-то слова из текстов отбрасывать (подумайте, каким образом это сделать - здесь на самом деле несколько вариантов концепций), а еще лемматизировать или хотя бы использовать стемминг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "\n",
    "# чтение файла\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "# обработка текста\n",
    "def preprocess_text(text, nlp):\n",
    "    doc = nlp(text) # токенизация\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc if token.is_alpha and token.lemma_ not in nlp.Defaults.stop_words] # заносим в список буквенные токены кроме стоп-слов\n",
    "    return set(lemmatized_tokens) # возвращает мн-во лемматизированных токенов\n",
    "\n",
    "# сравнение текстов\n",
    "def count_similarity(database_texts, new_text, nlp): # принимает database_texts (словарь из темы-текста), текст из нового файла, nlp\n",
    "    new_text_set = preprocess_text(new_text, nlp) # предобработка нового текста; вернется мн-во лемматизированных токенов\n",
    "\n",
    "    similarity = [] # сюда складываем кортежи, состящие из темы и степени близости (кол-ва пересекающихся слов) \n",
    "    for topic, database_text in database_texts.items(): # итерация по словарю database_texts\n",
    "        database_text_set = preprocess_text(database_text, nlp) # предобработка текста из базы\n",
    "        common_words = len(new_text_set.intersection(database_text_set)) # длина мн-ва пересекающихся слов (т.е. кол-во пересекающихся слов)\n",
    "        similarity.append((topic, common_words)) # заносим в список мн-во из тем и их \"степень близости\" c текстом\n",
    "    return max(similarity, key=lambda x: x[1]) # обращаемся к кортежу и возвращаем тему с макс. \"степенью близости\" и ее \"степень близости\"\n",
    "\n",
    "\n",
    "def comparing_texts():\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    database_texts_dir = 'C:/Users/Дина/Documents/универ/mag/database texts'\n",
    "    new_file_path = input(\"Введите название нового файла с текстом: \")\n",
    "\n",
    "    database_texts = {} # заводим словарь для базы: ключ - тема текста, значение - текст\n",
    "    for file_name in os.listdir(database_texts_dir): # итерируемся по названиям файлов в папке с базированными текстами\n",
    "        file_path = os.path.join(database_texts_dir, file_name) # полный путь к файлу (директория + имя файла)\n",
    "        topic = file_name.split('.')[0] # вытаскиваем тему текста из названия файла (я решила сделать так, чтобы немного проще жилось...)\n",
    "        text = read_file(file_path) # вызываем функцию чтения файла\n",
    "        database_texts[topic] = text # ключ - тема, значение - текст\n",
    "\n",
    "    new_text = read_file(new_file_path) # чиатем новый файл\n",
    "    topic, similarity = count_similarity(database_texts, new_text, nlp) # сравниваем текст из базы с новым\n",
    "    print(f'Текст больше всего похож на тему \"{topic}\"!')\n",
    "\n",
    "\n",
    "comparing_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 2\n",
    "Некоторые предлоги в русском языке могут управлять разными падежами (например, \"я еду в Лондон\" vs \"я живу в Лондоне\"). Давайте проанализируем эти предлоги и их падежи. Необходимо:\n",
    "\n",
    "- составить список таких предлогов (РГ-80 вам в помощь)\n",
    "- взять достаточно большой текст (можно большое художественное произведение)\n",
    "- сделать морфоразбор этого текста\n",
    "- Посчитать, как часто и какие падежи встречаются у слова, идущего после предлога.\n",
    "\n",
    "Примечания: во-первых, имейте в виду, что иногда после предлога могут идти самые неожиданные вещи: \"я что, должен ехать на, черт побери, северный полюс?\". Во-вторых, неплохо бы учитывать отсутствие пунктуации (конечно, в норме, как нам кажется, предлог обязательно требует зависимое, но! \"да иди ты на!\") Эти штуки можно отсеять, если просто учитывать только заранее определенные падежи, а не считать все, какие встретились (так и None можно огрести)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "prepositions = ['по', 'с', 'со', 'в', 'во', 'за', 'меж', 'между', 'промеж', 'промежду', 'на', 'о', 'об', 'под', 'подо', 'согласно']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3\n",
    "Представим, что у вас есть файл с разборами conllu (можете взять любой, например, тут). Нужно просмотреть все примеры предложений с тегом dislocated и тегом discourse: напишите скрипт, который будет читать файл, находить все такие предложения и печатать: 1) сам текст предложения 2) слово, имеющее искомый тег. Если тег не был найден в файле, нужно об этом сообщить. Постарайтесь оформить вывод таким образом, чтобы это было удобно читать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предложение: Короче : столько - то \" я как Я \" , остальное \" я как МЫ \" , а если только первое , то это — привет психические расстройства .\n",
      "Слово с тегом discourse: то\n",
      "Предложение: - Ой-ой-ой , а сам - то с мамой спишь каждый день , как маленький !\n",
      "Слово с тегом discourse: Ой-ой-ой\n",
      "Предложение: - Ой-ой-ой , а сам - то с мамой спишь каждый день , как маленький !\n",
      "Слово с тегом discourse: то\n",
      "Предложение: - Ну ты подожди ещё , у неё и дети будут .\n",
      "Слово с тегом discourse: Ну\n",
      "Предложение: И они на ковре вертолете сваливают сначала на Ямайку , а потом в Бразилию , ну это там где много диких обезьян в белых штанах .\n",
      "Слово с тегом discourse: ну\n"
     ]
    }
   ],
   "source": [
    "import pyconll\n",
    "\n",
    "\n",
    "text = pyconll.load_from_file('social.conllu')\n",
    "tags_found = 0\n",
    "\n",
    "for sentence in text:\n",
    "    for token in sentence:\n",
    "        if token.deprel == 'dislocated' or token.deprel == 'discourse':\n",
    "            tags_found += 1\n",
    "            print(f\"Предложение: {' '.join([token.form for token in sentence])}\\nСлово с тегом {token.deprel}: {token.form}\")\n",
    "\n",
    "if tags_found == 0:\n",
    "    print('Искомые теги не были найдены в файле.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 4\n",
    "Возьмите любой достаточно длинный (лучше новостной) текст. Любым известным инструментом извлеките именованные сущности из этого текста и выведите их списком по категориям (т.е. персоны вместе, локации вместе, организации вместе)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "organizations, locations, people = [], [], []\n",
    "\n",
    "with open('news.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "doc = nlp(text)\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'ORG':\n",
    "        organizations.append(ent.text)\n",
    "    elif ent.label_ == 'LOC':\n",
    "        locations.append(ent.text)\n",
    "    elif ent.label_ == 'PER':\n",
    "        people.append(ent.text)\n",
    "\n",
    "print(f'Вот найденные именованные сущности:\\n{set(organizations)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Именованные сущности не были найдены.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "named_entity_categories = {}\n",
    "\n",
    "with open('news.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ not in named_entity_categories:\n",
    "        named_entity_categories[ent.label_] = set()\n",
    "    named_entity_categories[ent.label_].add(ent.lemma_)\n",
    "\n",
    "if named_entity_categories:\n",
    "    for category, entity in named_entity_categories.items():\n",
    "        print(f'{category}: {\", \".join(entity)}')\n",
    "else:\n",
    "    print('Именованные сущности не были найдены.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
